#!/usr/bin/env bash
# check-links.sh – crawl every <loc> in a sitemap (or sitemap‑index) with lychee
set -euo pipefail

BASE_URL="${1:-https://developers.glean.com}"
DEEP_CHECK="${2:-false}"
SITEMAP_URL="${BASE_URL}/sitemap.xml"

tmp_dir="$(mktemp -d)"
url_list="${tmp_dir}/urls.txt"

echo "Fetching sitemap from: $SITEMAP_URL"
echo "Base URL: $BASE_URL"

# ── Fetch sitemap and extract all <loc> elements, namespace‑agnostic ──────────
if command -v xmllint >/dev/null 2>&1; then
  curl -sSL "$SITEMAP_URL" |
    xmllint --xpath "//*[local-name()='loc']/text()" - |
    tr ' ' '\n' |
    grep -E '^https?://' \
      >"$url_list"
else
  curl -sSL "$SITEMAP_URL" |
    python3 - "$url_list" <<'PY'
import sys, pathlib, xml.etree.ElementTree as ET
root = ET.fromstring(sys.stdin.read())
locs  = root.findall(".//{*}loc")
with pathlib.Path(sys.argv[1]).open("w") as f:
    for loc in locs:
        if loc.text and loc.text.startswith(("http://", "https://")):
            f.write(loc.text.strip() + "\n")
PY
fi

# ── Drop nested sitemaps so we check only page URLs ───────────────────────────
grep -vE '\.xml(\.gz)?$' "$url_list" >"${url_list}.pages"
mv "${url_list}.pages" "$url_list"

# ── Build lychee command with appropriate options ─────────────────────────────
lychee_cmd="lychee --verbose --max-redirects 10 --max-concurrency 8"

# Add exclusions for known problematic sites (using regex patterns)
lychee_cmd="$lychee_cmd --exclude ^https://community\.glean\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://support\.glean\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://app\.glean\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://.*-be\.glean\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://.*\.gleantest\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://.*internal\.company\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://codepen\.io/"

# Add exclusions for localhost and example URLs
lychee_cmd="$lychee_cmd --exclude ^https?://127\.0\.0\.1"
lychee_cmd="$lychee_cmd --exclude ^https?://localhost"
lychee_cmd="$lychee_cmd --exclude ^https?://company\.com/"
lychee_cmd="$lychee_cmd --exclude ^https?://example\.com/"
lychee_cmd="$lychee_cmd --exclude ^https?://example\.net/"
lychee_cmd="$lychee_cmd --exclude ^https?://your-domain\."
lychee_cmd="$lychee_cmd --exclude ^https?://your-org\."
lychee_cmd="$lychee_cmd --exclude ^https?://domain-be\."
lychee_cmd="$lychee_cmd --exclude ^https?://instance-be\."
lychee_cmd="$lychee_cmd --exclude ^https://.*\.glean\.engineering\.co\.in/"

# Add exclusions for external API endpoints that require authentication
lychee_cmd="$lychee_cmd --exclude ^https://accounts\.google\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://.*\.googleapis\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://docs\.google\.com/document/d/"
lychee_cmd="$lychee_cmd --exclude ^https://api\.atlassian\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://auth\.atlassian\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://.*\.atlassian\.net/"
lychee_cmd="$lychee_cmd --exclude ^https://portal\.azure\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://docs\.nvidia\.com/"
lychee_cmd="$lychee_cmd --exclude ^https://openid\.net/"
lychee_cmd="$lychee_cmd --exclude ^https://langchain-ai\.github\.io/"
lychee_cmd="$lychee_cmd --exclude ^https://picsum\.photos/"
lychee_cmd="$lychee_cmd --exclude ^https://docs\.lumapps\.com/"

# Add exclusions for GitHub URLs (will be handled separately if needed)
lychee_cmd="$lychee_cmd --exclude ^https://github\.com/gleanwork/"

# Add exclusions for dynamic OpenAPI fragment links that are generated by JavaScript
# Escape dots in base URL for regex and add full path patterns
base_url_escaped=$(echo "$BASE_URL" | sed 's/\./\\./g')
lychee_cmd="$lychee_cmd --exclude ^${base_url_escaped}/api/client-api/.*#"
lychee_cmd="$lychee_cmd --exclude ^${base_url_escaped}/api/indexing-api/.*#"

if [ "$DEEP_CHECK" = "true" ]; then
  echo "Running deep link check (crawls content of each page to find all links)..."
  lychee_cmd="$lychee_cmd --include-fragments --include-verbatim"
  
  echo "Restricting to base URL: $BASE_URL"
  lychee_cmd="$lychee_cmd --base-url $BASE_URL"
  
  # Pass URLs directly to lychee (not as a file) so it crawls their content
  echo "Found $(wc -l < "$url_list") pages to crawl..."
  $lychee_cmd $(cat "$url_list")
else
  echo "Running sitemap-only link check..."
  $lychee_cmd "$url_list"
fi

rm -rf "$tmp_dir"
