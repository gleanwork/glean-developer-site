#!/usr/bin/env bash
# ==================================================================================
# check-links.sh - Automated Link Checker for Documentation Sites
# ==================================================================================
#
# DESCRIPTION:
#   This script validates all links in a website by fetching its sitemap.xml and
#   using the 'lychee' link checker tool. It can operate in two modes:
#   - Quick mode: Only validates URLs listed in the sitemap
#   - Deep mode: Crawls the content of each page to find and validate ALL links
#
# USAGE:
#   ./check-links.sh [BASE_URL] [DEEP_CHECK]
#
# PARAMETERS:
#   BASE_URL    - The website URL to check (default: https://developers.glean.com)
#   DEEP_CHECK  - "true" for deep content crawling, "false" for sitemap-only
#                 (default: false)
#
# EXAMPLES:
#   ./check-links.sh                                    # Check production site (quick)
#   ./check-links.sh https://developers.glean.com true  # Deep check of production
#   ./check-links.sh http://localhost:3000             # Check local build
#
# HOW IT WORKS:
#   1. SITEMAP FETCHING:
#      - Downloads sitemap.xml from the base URL
#      - Extracts all <loc> URLs using xmllint (or Python fallback)
#      - Filters out nested sitemap references (*.xml files)
#
#   2. LINK CHECKER CONFIGURATION:
#      - Uses bash arrays to safely build the lychee command
#      - Configures rate limiting and retry behavior
#      - Applies extensive exclusion patterns for:
#        * Internal Glean sites that require authentication
#        * External sites with auth requirements (Google, Atlassian, Azure, etc.)
#        * Localhost and example/placeholder URLs
#        * Dynamic API documentation anchors (generated by JavaScript)
#
#   3. CHECKING MODES:
#      - QUICK MODE (default): Validates only the URLs found in sitemap
#        Fast, checks if pages exist and are accessible
#
#      - DEEP MODE: Crawls each page's content and validates all links found
#        Comprehensive but slower, finds broken links within page content
#
# REQUIREMENTS:
#   - lychee: Link checker tool (install via: brew install lychee)
#   - xmllint or Python 3: For parsing sitemap XML
#   - curl: For fetching the sitemap
#
# EXCLUSIONS:
#   The script excludes many problematic URL patterns including:
#   - Authentication-required endpoints (app.glean.com, portal.azure.com, etc.)
#   - Example/placeholder domains (example.com, your-domain.*, etc.)
#   - Dynamic JavaScript-generated anchors in API docs
#   - Internal development/test domains
#
# EXIT CODES:
#   0 - Success (no broken links found)
#   Non-zero - Failure (broken links detected or script error)
#
# ==================================================================================

set -euo pipefail

# Parse command-line arguments with defaults
BASE_URL="${1:-https://developers.glean.com}"
DEEP_CHECK="${2:-false}"
SITEMAP_URL="${BASE_URL}/sitemap.xml"

# Create temporary directory for processing
tmp_dir="$(mktemp -d)"
url_list="${tmp_dir}/urls.txt"

echo "Fetching sitemap from: $SITEMAP_URL"
echo "Base URL: $BASE_URL"

# ── Fetch sitemap and extract all <loc> elements, namespace‑agnostic ──────────
if command -v xmllint >/dev/null 2>&1; then
  curl -sSL "$SITEMAP_URL" |
    xmllint --xpath "//*[local-name()='loc']/text()" - |
    tr ' ' '\n' |
    grep -E '^https?://' \
      >"$url_list"
else
  curl -sSL "$SITEMAP_URL" |
    python3 - "$url_list" <<'PY'
import sys, pathlib, xml.etree.ElementTree as ET
root = ET.fromstring(sys.stdin.read())
locs  = root.findall(".//{*}loc")
with pathlib.Path(sys.argv[1]).open("w") as f:
    for loc in locs:
        if loc.text and loc.text.startswith(("http://", "https://")):
            f.write(loc.text.strip() + "\n")
PY
fi

# ── Drop nested sitemaps so we check only page URLs ───────────────────────────
grep -vE '\.xml(\.gz)?$' "$url_list" >"${url_list}.pages"
mv "${url_list}.pages" "$url_list"

# ── Build lychee command with appropriate options ─────────────────────────────
# Use an array to properly handle arguments and avoid word splitting issues
# This prevents problems with spaces and special characters in arguments
lychee_cmd=(
  lychee
  --verbose                # Show detailed progress
  --max-redirects 10       # Follow up to 10 redirects
  --max-concurrency 8      # Limit concurrent requests to avoid rate limiting
)

# Add exclusions for known problematic sites (using regex patterns)
# These are internal Glean sites that require authentication or are private
lychee_cmd+=(
  --exclude "^https://community\.glean\.com/"      # Community forum (auth required)
  --exclude "^https://support\.glean\.com/"        # Support portal (auth required)
  --exclude "^https://app\.glean\.com/"            # Main app (auth required)
  --exclude "^https://.*-be\.glean\.com/"          # Backend endpoints (private)
  --exclude "^https://.*\.gleantest\.com/"         # Test environments
  --exclude "^https://.*internal\.company\.com/"   # Example internal domains
  --exclude "^https://codepen\.io/"                # CodePen embeds (often fail)
)

# Add exclusions for localhost and example/placeholder URLs
# These appear in documentation as examples and aren't real links
lychee_cmd+=(
  --exclude "^https?://127\.0\.0\.1"               # Localhost IP
  --exclude "^https?://localhost"                  # Localhost hostname
  --exclude "^https?://company\.com/"              # Example domain in docs
  --exclude "^https?://example\.com/"              # RFC example domain
  --exclude "^https?://example\.net/"              # RFC example domain
  --exclude "^https?://your-domain\."              # Placeholder in docs
  --exclude "^https?://your-org\."                 # Placeholder in docs
  --exclude "^https?://domain-be\."                # Placeholder backend URL
  --exclude "^https?://instance-be\."              # Placeholder backend URL
  --exclude "^https://.*\.glean\.engineering\.co\.in/" # Internal dev domain
)

# Add exclusions for external API endpoints that require authentication
# These external services often block automated link checkers or require auth
lychee_cmd+=(
  --exclude "^https://accounts\.google\.com/"      # Google auth portal
  --exclude "^https://.*\.googleapis\.com/"        # Google APIs (auth required)
  --exclude "^https://docs\.google\.com/document/d/" # Private Google Docs
  --exclude "^https://api\.atlassian\.com/"        # Atlassian APIs
  --exclude "^https://auth\.atlassian\.com/"       # Atlassian auth
  --exclude "^https://.*\.atlassian\.net/"         # Atlassian instances
  --exclude "^https://portal\.azure\.com/"         # Azure portal (auth)
  --exclude "^https://docs\.nvidia\.com/"          # NVIDIA docs (often blocks)
  --exclude "^https://openid\.net/"                # OpenID site (flaky)
  --exclude "^https://langchain-ai\.github\.io/"   # LangChain docs (flaky)
  --exclude "^https://picsum\.photos/"             # Random image service
  --exclude "^https://docs\.lumapps\.com/"        # LumApps docs (auth)
)

# Add exclusions for GitHub URLs (will be handled separately if needed)
lychee_cmd+=( --exclude "^https://github\.com/gleanwork/" )

# Add exclusions for dynamic OpenAPI fragment links that are generated by JavaScript
# The API documentation pages use JavaScript to dynamically generate anchor links
# for request/response fields (e.g., #request-iconurl). These don't exist in the
# static HTML, so the link checker would report them as broken even though they
# work in the browser. We exclude all fragment links (#) in API docs.
# Escape dots in base URL for regex and add full path patterns
base_url_escaped=$(echo "$BASE_URL" | sed 's/\./\\./g')
lychee_cmd+=(
  --exclude "^${base_url_escaped}/api/client-api/.*#"    # Client API anchors
  --exclude "^${base_url_escaped}/api/indexing-api/.*#"  # Indexing API anchors
)

if [ "$DEEP_CHECK" = "true" ]; then
  echo "Running deep link check (crawls content of each page to find all links)..."
  lychee_cmd+=(
    --include-fragments
    --include-verbatim
  )
  
  echo "Restricting to base URL: $BASE_URL"
  lychee_cmd+=( --base-url "$BASE_URL" )
  
  # Pass URLs directly to lychee (not as a file) so it crawls their content
  echo "Found $(wc -l < "$url_list") pages to crawl..."
  "${lychee_cmd[@]}" $(cat "$url_list")
else
  echo "Running sitemap-only link check..."
  "${lychee_cmd[@]}" "$url_list"
fi

rm -rf "$tmp_dir"
